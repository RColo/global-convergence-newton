\documentclass{article}

% ready for submission
\usepackage{neurips_2022}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{amsmath}

\title{Global Convergence Newton}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{
  Raffael Colonnello\\
  University of Basel\\
  \texttt{Raffael.Colonnello@unibas.ch} \\
  \And
  Fynn Gohlke\\
  University of Basel\\
  \texttt{Fynn.Gohlke@stud.unibas.ch} \\
  \AND
  Benedikt Heuser\\
  University of Basel\\
  \texttt{ben.heuser@unibas.ch} \\
}

\begin{document}

\maketitle


\begin{abstract}
  The abstract paragraph should be indented \nicefrac{1}{2}~inch (3~picas) on
  both the left- and right-hand margins. Use 10~point type, with a vertical
  spacing (leading) of 11~points.  The word \textbf{Abstract} must be centered,
  bold, and in point size 12. Two line spaces precede the abstract. The abstract
  must be limited to one paragraph.
\end{abstract}

\section{Introduction}
In this paper we consider problems of the

\begin{equation}
  \min_{x \in \mathbb{R}^d} f(\mathbf{x})
\end{equation}

where $f: \mathbb{R}^d \rightarrow \mathbb{R}$ is a twice-differentiable function. First-order optimization methods are widely used for such problems due to their low per-iteration computational cost and their suitability for parallelization they often suffer from slow convergence for ill-conditioned objective functions [TODO:REFERENCE].
Newton's method is a popular optimization algorithm that is commonly used to solve optimization
problems. It is a second-order optimization algorithm since it uses second-order information of the
objective function. Newton's method is known to have fast local convergence guarantees for convex
functions. However, the global convergence properties of Newton's method are still an active area of
research [TODO: REFERENCE]. The purpose of this project is to survey and analyze various strategies to achieve global
convergence. In contrast to first-order methods such as gradient descent, second-order methods such as Newton's method can achieve much faster convergence when presented with ill conditioned Hessians by transferring the problem into a more isotropic optimization problem at the cost of an increase to cubic run time. Newton's method yields local quadratic convergence if $f$ is twice differentiable (or we have suitable regularity conditions), which degrades to sublinear convergence outside of the local regions.

In this paper, we explore the theoretical foundations of several Newton-type methods that achieve different global convergence guarantees, compare their performance in a classification-type problem for two Loss function on four different datasets.
Finally we will propose two modifications of the algorithms to achieve an increase in runtime, by either coupling the Newton-type method with a conjugate gradient method for Hessian vector multiplication or Strassens algorithm for fast matrix inversion.

This sentence will be cited by the sources so i can test if bibtex is working properly \cite{hanzely2022damped} 

\section{Background}
\subsection{Loss function and Datasets}
Let 
$X = 
\begin{bmatrix}
\hdots x_1^\top \hdots \\
\vdots \\
\hdots x_i^\top \hdots\\
\vdots \\
\hdots x_n^\top \hdots
\end{bmatrix}
\in \mathbb{R}^{n \times d}$ \quad be the set of data for $n$ datapoints with $d$ features, i.e. $x_i \in \mathbb{R}^d$ and labels $y^\top = \begin{bmatrix} y_1,...,y_n\end{bmatrix}$\\
For $\sigma(z) := \frac{\exp(z)}{1+\exp(z)}$ the loss functions w.r.t. weights $\omega$ is given by
\begin{align}
L_1(\omega) &= -\frac{1}{n} \sum_{i=1}^{n} \Big( y_i \log \hat{y}_i + (1 - y_i) \log (1 - \hat{y}_i) \Big), \quad \hat{y}_i = \sigma(z_i^\top \omega) \\
L_2(\omega) &= \frac{1}{n} \sum_{i=1}^{n} \log\big(1 + \exp(-y_i z_i^\top \omega)\big) + r(\omega), \quad
r(\omega) = \lambda \sum_{j=1}^{d} \frac{\alpha \omega _j^2}{1 + \alpha \omega _j^2} \\
\end{align}
which yields the two optimization problems
\begin{align}
  \min _{\omega} L_1(\omega)\\
  \min _{\omega} L_2(\omega)
\end{align}
The corresponding gradients of $L_i$ are
\begin{align}
\nabla L_1(x) &= \frac{1}{n} X^\top (\hat{y} - y) \\
\nabla L_2(x) &= -\frac{1}{n} X^\top \big(y \odot \sigma(-y \odot (X\omega))\big) + \nabla r(x)\\
\text{with}\quad & \nabla r(\omega)^\top = \lambda \cdot \left[ \frac{2\alpha \omega _1}{(1 + \alpha \omega _1^2)^2},...,\frac{2\alpha \omega _d}{(1 + \alpha \omega _d^2)^2} \right] \quad \text{and $\sigma(\ \cdot \ )$ applied  elementwise}\notag\\
\text{and} \ \ \odot& \ \ \text{denotes the entrywise multiplication of vectors.}\notag
\end{align}
Differentiating again yields the Hessians
\begin{align}
\nabla^2 L_1(\omega) &= \frac{1}{n} X^\top D X\\
\nabla^2 L_2(\omega) &= \frac{1}{n} X^\top D X + \nabla^2 r(\omega), \nabla^2, \quad
r(\omega) = \operatorname{diag}\Big(\lambda \frac{2\alpha (1 - 3\alpha \omega_j^2)}{(1 + \alpha \omega_j^2)^3}\Big)
\end{align}
where the diagonal matrix $D$ has entries
\begin{align}
D_{ii} &= \hat{y}_i (1 - \hat{y}_i)=  \sigma(-y_i z_i^\top \omega) \big(1 - \sigma(-y_i z_i^\top \omega)\big),\quad 
\end{align}
Observation:\\
Since $\log(\hat{y_i}),\log(1-\hat{y_i})$ are concave on $(0,\infty)$ it follows that $-\log(\hat{y_i}),-\log(1-\hat{y_i})$ are convex and thus $L_1$ is a linear combination of convex functions (which is agian convex). Meanwhile $L_2$ is not guaranteed to be convex due to the non-convex regularization term $r(\omega)$. 

\subsection{Classic Newton's Method}

The classical origin of Newton's method is as an algorithm for finding the roots of functions. In this paper it is used to find the roots $x^{*}$ of $\nabla (f(x)) \hspace{0.1cm} s.t.\nabla(f(x^{*})) = 0$ and $x^{*}$ a local minimum of $f$. Newton's method uses the update rule [REFERENCE]:
\begin{equation}
  \mathbf{x}_{k+1} = \mathbf{x}_k - \eta (\nabla^2 f(\mathbf{x}_k))^{-1} \nabla f(\mathbf{x}_k)
\end{equation}

The inverse Hessian can be interpreted as transforming the gradient landscape to be more isotropic, thereby improving the conditioning of the problem. 

\subsection{Cubic Newton}

The cubic Newton method was one of the first to achieve a good complexity guarantee globally [REFERENCE TO DO: What convergence rate exactly?]. It is based on cubic regularization and uses the update rule:

\begin{equation}
\mathbf{x}_{k+1} = \mathbf{x}_{k} - ( \nabla^2 f(\mathbf{x}_k) + H ||\mathbf{x}_{k+1} - \mathbf{x}_{k}||\mathbf{I})^{-1} \nabla f(\mathbf{x}_k)
\end{equation}

\subsection{Levenberg and Marquardt method}

The Levenberg-Marquardt's algorithm [REFERENCE] is an early form of regularized Newton's method that modifies the Hessian. For ill conditioned (or singular) H this can increase conergence (or make the problem solvable as $H + \lambda I$ is always invertible for sufficiently large $eig(H)> - \lambda, \lambda > 0$).he update rule is:
\begin{equation}
\mathbf{x}_{k+1} = \mathbf{x}_{k} - ( \nabla^2 f(\mathbf{x}_k) + \lambda_k \mathbf{I})^{-1} \nabla f(\mathbf{x}_k)
\end{equation}

\subsection{Regularized Newton}

In their 2023 article Michenko presents a variation of Newton's method that uses the update rule \cite{mishchenko2023regularized}:

\begin{equation}
\mathbf{x}_{k+1} = \mathbf{x}_{k} - ( \nabla^2 f(\mathbf{x}_k) + \sqrt{ H ||\nabla f(\mathbf{x}_k)||\mathbf{I}})^{-1} \nabla f(\mathbf{x}_k)
  \label{eq:regularized-newton}
\end{equation}

where $H > 0$ is a constant. The convergence rate of this algorithm is $\mathcal{O}(\frac{1}{k^2})$. This method uses an adaptive variant of the Levenberg-Marquardt regularization. 


\subsection{TODO}
Jorge Nocedal \& Stephen J. Wright, Numerical Optimization (2nd ed), Springer (2006). Chapter 7 (Newton-CG methods for large-scale optimization) Section 7.2: "Newton-CG Algorithm"\\

\subsection{Inexact Newton Method}
Given that Newton has cubic complexity we now outline how we aim to reduce the runtime by extending CG and MINRES methods to the Newton-type methods described in our paper. In order for the modified algorithms to inherit the convergence guarantees of the algorithms we want to approximate $p$ s.t.
$$ \| H p + \nabla f \| \leq \epsilon \hspace{0.2cm} \text{(absolute tolerance)} < \epsilon = 10^{-8} $$
Since $H_{1,2} = \nabla ^2 L_{1,2}$ are clearly symmetric (as both $X^\top D X$ and $\nabla ^2 r(x)$ are) we can apply the conjugate gradient method 
if the H is positive definite or have to fall back on MINRES if it is not pd. Positive definiteness depends on the data matrix and the regularizer curvature.
[TODO: runtime for MINRES and CG]\\
Every iteration of Vanilla Newton takes O($n^3$) per iteration because inversion of the Hessian costs O($n^3$).\\
for symmetric applying CG to newton drops the effort for conversion down to 
$$O(k\cdot n^2) = O(\sqrt{\kappa}\log \left( 1/\epsilon\right) \cdot n^2)$$ where $\kappa(H) = \frac{\lambda_{max}(H)}{\lambda_{max}(H)}$\\
Precondition with SSOR to reduce condition number.
\bibliographystyle{unsrt}
\bibliography{refs}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Checklist}


%%% BEGIN INSTRUCTIONS %%%
The checklist follows the references.  Please
read the checklist guidelines carefully for information on how to answer these
questions.  For each question, change the default \answerTODO{} to \answerYes{},
\answerNo{}, or \answerNA{}.  You are strongly encouraged to include a {\bf
justification to your answer}, either by referencing the appropriate section of
your paper or providing a brief inline description.  For example:
\begin{itemize}
  \item Did you include the license to the code and datasets? \answerYes{See Section}
  \item Did you include the license to the code and datasets? \answerNo{The code and the data are proprietary.}
  \item Did you include the license to the code and datasets? \answerNA{}
\end{itemize}
Please do not modify the questions and only use the provided macros for your
answers.  Note that the Checklist section does not count towards the page
limit.  In your paper, please delete this instructions block and only keep the
Checklist section heading above along with the questions/answers below.
%%% END INSTRUCTIONS %%%


\begin{enumerate}


\item For all authors...
\begin{enumerate}
  \item Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?
    \answerTODO{}
  \item Did you describe the limitations of your work?
    \answerTODO{}
  \item Did you discuss any potential negative societal impacts of your work?
    \answerTODO{}
  \item Have you read the ethics review guidelines and ensured that your paper conforms to them?
    \answerTODO{}
\end{enumerate}


\item If you are including theoretical results...
\begin{enumerate}
  \item Did you state the full set of assumptions of all theoretical results?
    \answerTODO{}
        \item Did you include complete proofs of all theoretical results?
    \answerTODO{}
\end{enumerate}


\item If you ran experiments...
\begin{enumerate}
  \item Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)?
    \answerTODO{}
  \item Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)?
    \answerTODO{}
        \item Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)?
    \answerTODO{}
        \item Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)?
    \answerTODO{}
\end{enumerate}


\item If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
\begin{enumerate}
  \item If your work uses existing assets, did you cite the creators?
    \answerTODO{}
  \item Did you mention the license of the assets?
    \answerTODO{}
  \item Did you include any new assets either in the supplemental material or as a URL?
    \answerTODO{}
  \item Did you discuss whether and how consent was obtained from people whose data you're using/curating?
    \answerTODO{}
  \item Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content?
    \answerTODO{}
\end{enumerate}


\item If you used crowdsourcing or conducted research with human subjects...
\begin{enumerate}
  \item Did you include the full text of instructions given to participants and screenshots, if applicable?
    \answerTODO{}
  \item Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable?
    \answerTODO{}
  \item Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation?
    \answerTODO{}
\end{enumerate}


\end{enumerate}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\appendix


\section{Appendix}


Optionally include extra information (complete proofs, additional experiments and plots) in the appendix.
This section will often be part of the supplemental material.


\end{document}
